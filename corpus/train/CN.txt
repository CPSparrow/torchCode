业内主要的序列转导模型基于复杂的递归或卷积神经网络，其中包括编码器和解码器。
性能最佳的模型还通过一个注意力机制连接编码器和解码器。
我们提出了一种新的简单网络架构，即 Transformer，它完全基于注意力机制，完全省去了递归和卷积。
对两项机器翻译任务的实验表明，这些模型在质量上更好，同时并行化程度更高，并且需要的训练时间明显减少。
我们的模型在 WMT 2014 英语到德语的翻译任务中实现了 28.4 BLEU，比现有的最佳结果（包括ensembles）提高了 2 BLEU 以上。
在 WMT 2014 英语到法语翻译任务中，我们的模型在 8 个 GPU 上训练 3.5 天后，建立了一个新的单模型最先进的 BLEU 分数，即 41.8，这只是文献中最佳模型的训练成本的一小部分。
我们表明，Transformer 成功地将其应用于具有大量和有限训练数据的英语选区解析，从而很好地推广到其他任务。
特别的，循环神经网络，长短期记忆，门控循环神经网络这三种，已经被牢固地确立为序列建模和转导问题（如语言建模和机器翻译）的最佳方法。
此后，人们不断努力突破递归语言模型和编码器-解码器架构的界限。
递归模型通常沿输入和输出序列的符号位置进行 factor 计算。将位置与计算时间中的步骤对齐，它们生成一系列隐藏状态 ht，作为前一个隐藏状态 ht−1 和位置 t 输入的函数。
这种固有的 Sequences 性质排除了训练样本中的并行化，这在较长的序列长度下变得至关重要，因为内存约束限制了样本之间的批处理。
最近的工作通过因式分解技巧和条件计算实现了计算效率的显著提高，同时也提高了后者的模型性能。
但是，顺序计算的基本约束仍然存在。
注意力机制已成为各种任务中引人注目的序列建模和转导模型不可或缺的一部分，允许对依赖关系进行建模，而不考虑它们在输入或输出序列中的距离。
然而，除了少数情况外，这种注意力机制都与循环网络结合使用。
在这项工作中，我们提出了 Transformer，这是一种避免重复出现的模型架构，同时完全依赖注意力机制来预测输入和输出之间的全局依赖关系。
Transformer 允许显著提高并行化水平，并且在 8 个 P100 GPU 上经过短短 12 小时的训练后，就可以达到翻译质量的新高度。
减少顺序计算的目标也构成了扩展神经 GPU、ByteNet 和 ConvS2S 的基础，所有这些 GPU 都使用卷积神经网络作为基本构建块，并行计算所有输入和输出位置的隐藏表示。
在这些模型中，关联来自两个任意输入或输出位置的信号所需的操作数随着位置之间的距离增加而增加，对于 ConvS2S 呈线性增长，对于 ByteNet 呈对数增长。
这使得学习远程位置之间的依赖关系变得更加困难。
在 Transformer 中，这被减少到恒定数量的操作，尽管代价是由于平均注意力加权位置而降低了有效分辨率，我们用 Multi-Head Attention 来抵消这种效果，如 3.2 节所述。
