{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## todo_list\n",
    "1. [ ] 是否已经实现了post_norm"
   ],
   "id": "76d891be6efe51d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. 导入相关的库",
   "id": "92bcabb2405d0565"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T02:15:41.372183Z",
     "start_time": "2024-10-12T02:15:40.648005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "id": "18508b8c6c5a456a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. 实现相关的函数",
   "id": "7ca80e6950ecba9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T02:15:41.381660Z",
     "start_time": "2024-10-12T02:15:41.376927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clones(module, N: int):\n",
    "\treturn nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "\tdef __init__(self, size, eps: float = 1e-5):\n",
    "\t\t# 这里eps的数值参考了pytorch的transformer实现\n",
    "\t\tsuper(LayerNorm, self).__init__()\n",
    "\t\tself.a_2 = nn.Parameter(torch.ones(size))\n",
    "\t\tself.b_2 = nn.Parameter(torch.zeros(size))\n",
    "\t\tself.eps = eps\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tmean = x.mean(-1, keepdim=True)\n",
    "\t\tstd = x.std(-1, keepdim=True)\n",
    "\t\treturn self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "\tdef __init__(self, size, dropout):\n",
    "\t\tsuper(SublayerConnection, self).__init__()\n",
    "\t\tself.norm = LayerNorm(size)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\n",
    "\tdef forward(self, x, sublayer):\n",
    "\t\treturn x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\tdef __init__(self, d_model: int, vocab_size: int):\n",
    "\t\tsuper(Generator, self).__init__()\n",
    "\t\tself.proj = nn.Linear(d_model, vocab_size)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\treturn F.log_softmax(self.project(x), dim=1)\n",
    "\n"
   ],
   "id": "7d7245ebf97920ab",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. 实现transformer",
   "id": "f8d2efc21ed1f2bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T02:15:41.432379Z",
     "start_time": "2024-10-12T02:15:41.425723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Transformer(nn.Module):\n",
    "\tdef __init__(self, encoder, decoder, src_emb, tgt_emb, generator):\n",
    "\t\tsuper(Transformer, self).__init__()\n",
    "\t\tself.encoder = encoder\n",
    "\t\tself.decoder = decoder\n",
    "\t\tself.src_emb = src_emb\n",
    "\t\tself.tgt_emb = tgt_emb\n",
    "\t\tself.generator = generator\n",
    "\t\n",
    "\tdef forward(self, src, tgt, src_mask, tgt_mask):\n",
    "\t\tr\"\"\"\n",
    "\t\tused while training\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\t\n",
    "\tdef encode(self, src, src_mask):\n",
    "\t\treturn self.encoder(self.src_emb(src), src_mask)\n",
    "\t\n",
    "\tdef decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "\t\treturn self.decoder(self.tgt_emb(tgt), memory, src_mask, tgt_mask)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\tdef __init__(self, layer, N: int):\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\t\tself.layers = clones(layer, N)\n",
    "\t\tself.norm = LayerNorm(layer.size)\n",
    "\t\n",
    "\tdef forward(self, x, mask):\n",
    "\t\tfor layer in layers:\n",
    "\t\t\tx = layer(x, mask)\n",
    "\t\treturn self.norm(x)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "\tdef __init__(self, size, self_attn, feed_forward, dropout=0.1):\n",
    "\t\tsuper(EncoderLayer, self).__init__()\n",
    "\t\tself.size = size\n",
    "\t\tself.self_attn = self_attn\n",
    "\t\tself.feed_forward = feed_forward\n",
    "\t\tself.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "\t\n",
    "\tdef forward(self, x, mask):\n",
    "\t\tx = self.sublayer[0](x, lambda src: self.self_attn(src, src, src, mask))\n",
    "\t\treturn self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self, layer, N):\n",
    "\t\tsuper(Decoder, self).__init__()\n",
    "\t\tself.layers = clones(layer, N)\n",
    "\t\tself.norm = LayerNorm(layer.size)\n",
    "\t\n",
    "\tdef forward(self, x, memory, src_mask, tgt_mask):\n",
    "\t\tfor layer in layers:\n",
    "\t\t\tx = layer(x, memory, src_mask, tgt_mask)\n",
    "\t\treturn self.norm(x)\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "\tdef __init__(self, size, self_attn, cross_attn, feed_forward, dropout=0.1):\n",
    "\t\tsuper(DecoderLayer, self).__init__()\n",
    "\t\tself.size = size\n",
    "\t\tself.self_attn = self_attn\n",
    "\t\tself.cross_attn = cross_attn\n",
    "\t\tself.feed_forward = feed_forward\n",
    "\t\tself.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\t\n",
    "\tdef forward(self, x, memory, cross_mask, tgt_mask):\n",
    "\t\tm = memory\n",
    "\t\tx = self.sublayer[0](x, lambda tgt: self.self_attn(tgt, tgt, tgt, tgt_mask))\n",
    "\t\tx = self.sublayer[1](x, lambda tgt: self.cross_attn(tgt, m, m, cross_mask))\n",
    "\t\treturn self.sublayer[2](x, self.feed_forward)"
   ],
   "id": "23dd81e8a2b6dd8a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. 实现attention",
   "id": "9dacefa8c0cb5352"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T02:15:41.489876Z",
     "start_time": "2024-10-12T02:15:41.480802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def subsequent_mask(seq_size: int):\n",
    "\tattn_shape = (1, seq_size, seq_size)\n",
    "\tmask = (torch.triu(torch.ones(attn_shape), diagonal=1).\n",
    "\t\t\ttype(torch.uint8))\n",
    "\treturn mask == 0\n",
    "\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "\td_k = query.size(-1)\n",
    "\tscores = torch.matmul(query, key.transpose(-2, -1) / math.sqrt(d_k))\n",
    "\tif mask is not None:\n",
    "\t\tscores = scores.masked_fill(mask == 0, 1e-9)\n",
    "\tp_attn = scores.softmax(dim=-1)\n",
    "\tif droput is not None:\n",
    "\t\tp_attn = dropout(p_attn)\n",
    "\treturn torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "class MHA(nn.Module):\n",
    "\tdef __init__(self, n_head: int, d_model: int, dropout: float = 0.1):\n",
    "\t\tsuper(MHA, self).__init__()\n",
    "\t\tassert d_model % h == 0\n",
    "\t\tself.d_k = d_model // n_head\n",
    "\t\tself.n_head = n_head\n",
    "\t\t# 提问：为什么是clone(..., 4)?\n",
    "\t\t# 回答：四个线性映射，前三个对应 q k \n",
    "\t\t# 提问：在这里的实现中是简单的做了一个大linear再切分。这是否与\n",
    "\t\t#      通过n_head的多个linear等价？\n",
    "\t\tself.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "\t\tself.attn = None\n",
    "\t\tself.dropout = nn.Dropout(p=dropout)\n",
    "\t\n",
    "\tdef forward(self, query, key, value, mask=None):\n",
    "\t\tif mask is not None:\n",
    "\t\t\tmask = mask.unsqueeze(1)\n",
    "\t\tbatch_num = query.size(0)\n",
    "\t\t\n",
    "\t\tquery, key, value = [\n",
    "\t\t\tlinear(x).view(batch_num, -1, self.h, self.d_k).transpose(1, 2)\n",
    "\t\t\tfor linear, x in zip(self.linears, (query, key, value))\n",
    "\t\t]\n",
    "\t\t\n",
    "\t\tx, self.attn = attention(\n",
    "\t\t\tquery, key, value, mask=mask, dropout=self.dropout\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tx = (\n",
    "\t\t\tx.transpose(1, 2)\n",
    "\t\t\t.contiguous()\n",
    "\t\t\t.view(batch_num, -1, self.h, self.d_k)\n",
    "\t\t)\n",
    "\t\tdel query\n",
    "\t\tdel key\n",
    "\t\tdel value\n",
    "\t\t\n",
    "\t\treturn self.linears[-1](x)"
   ],
   "id": "4ccff8b6b3b13391",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. 实现FFN\n",
    "> 如果看源码的话,可以看到linear的参数都已经做了初始化而且bias默认为True"
   ],
   "id": "f115cc6a975245d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T02:15:41.542127Z",
     "start_time": "2024-10-12T02:15:41.535879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FFN(nn.Module):\n",
    "\tdef __init__(self, d_model: int, d_ffn: int, dropout=0.1):\n",
    "\t\tsuper(FFN, self).__init__()\n",
    "\t\t# 注意到bias：default = true\n",
    "\t\tself.w_1 = nn.Linear(d_model, d_ffn)\n",
    "\t\tself.w_2 = nn.Linear(d_ffn, d_model)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.w_2(self.dropout(self.w_1(x).relu()))"
   ],
   "id": "ee9abb03911c3add",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6. 实现 embedding 和 softmax \n",
    "> 参考源码，这里调整为log_softmax"
   ],
   "id": "6c28f9540ffb5c26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T02:15:41.591295Z",
     "start_time": "2024-10-12T02:15:41.588710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Embedding(nn.Module):\n",
    "\tdef __init__(self, vocab_size: int, d_model: int):\n",
    "\t\tsuper(Embedding, self).__init__()\n",
    "\t\tself.emb = nn.Embedding(vocab_size, d_model)\n",
    "\t\tself.d_model = d_model\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.emb(x) * math.sqrt(self.d_model)\n",
    "\n",
    "\n",
    "class PosEmb(nn.Module):\n",
    "\tdef __init__(self, d_model: int, max_len: int = 1500, dropout: float = 0.1):\n",
    "\t\tsuper(PosEmb, self).__init__()\n",
    "\t\tself.dropout = nn.Dropout(p=dropout)\n",
    "\t\t\n",
    "\t\tpe = torch.zeros(max_len)"
   ],
   "id": "f53162223ccfa8c9",
   "outputs": [],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
