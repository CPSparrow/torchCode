{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## todo_list\n",
    "1. [ ] 是否已经实现了post_norm"
   ],
   "id": "76d891be6efe51d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. 导入相关的库",
   "id": "92bcabb2405d0565"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:05:32.112128Z",
     "start_time": "2024-10-09T15:05:32.109957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "id": "18508b8c6c5a456a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. 实现相关的函数",
   "id": "7ca80e6950ecba9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:05:32.159564Z",
     "start_time": "2024-10-09T15:05:32.155255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clones(module, N):\n",
    "\treturn nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "\tdef __init__(self, size, eps=1e-6):\n",
    "\t\tsuper(LayerNorm, self).__init__()\n",
    "\t\tself.a_2 = nn.Parameter(torch.ones(size))\n",
    "\t\tself.b_2 = nn.Parameter(torch.zeros(size))\n",
    "\t\tself.eps = eps\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tmean = x.mean(-1, keepdim=True)\n",
    "\t\tstd = x.std(-1, keepdim=True)\n",
    "\t\treturn self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "\tdef __init__(self, size, dropout):\n",
    "\t\tsuper(SublayerConnection, self).__init__()\n",
    "\t\tself.norm = LayerNorm(size)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\n",
    "\tdef forward(self, x, sublayer):\n",
    "\t\treturn x + self.dropput(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\tdef __init__(self, d_model, vocab):\n",
    "\t\tsuper(Generator, self).__init__()\n",
    "\t\tself.proj = nn.Linear(d_model, vocab)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\treturn F.log_softmax(self.project(x), dim=1)\n",
    "\n"
   ],
   "id": "7d7245ebf97920ab",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. 实现transformer",
   "id": "f8d2efc21ed1f2bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:05:32.216510Z",
     "start_time": "2024-10-09T15:05:32.207050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Transformer(nn.Module):\n",
    "\tdef __init__(self, encoder, decoder, src_emb, tgt_emb, generator):\n",
    "\t\tsuper(Transformer, self).__init__()\n",
    "\t\tself.encoder = encoder\n",
    "\t\tself.decoder = decoder\n",
    "\t\tself.src_emb = src_emb\n",
    "\t\tself.tgt_emb = tgt_emb\n",
    "\t\tself.generator = generator\n",
    "\t\n",
    "\tdef forward(self, src, tgt, src_mask, tgt_mask):\n",
    "\t\tr\"\"\"\n",
    "\t\tused while training\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\t\n",
    "\tdef encode(self, src, src_mask):\n",
    "\t\treturn self.encoder(self.src_emb(src), src_mask)\n",
    "\t\n",
    "\tdef decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "\t\treturn self.decoder(self.tgt_emb(tgt), memory, src_mask, tgt_mask)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\tdef __init__(self, layer, N):\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\t\tself.layers = clones(layer, N)\n",
    "\t\tself.norm = LayerNorm(layer.size)\n",
    "\t\n",
    "\tdef forward(self, x, mask):\n",
    "\t\tfor layer in layers:\n",
    "\t\t\tx = layer(x, mask)\n",
    "\t\treturn self.norm(x)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "\tdef __init__(self, size, self_attn, feed_forward, dropout):\n",
    "\t\tsuper(EncoderLayer, self).__init__()\n",
    "\t\tself.size = size\n",
    "\t\tself.self_attn = self_attn\n",
    "\t\tself.feed_forward = feed_forward\n",
    "\t\tself.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "\t\n",
    "\tdef forward(self, x, mask):\n",
    "\t\tx = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "\t\treturn self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self, layer, N):\n",
    "\t\tsuper(Decoder, self).__init__()\n",
    "\t\tself.layers = clones(layer, N)\n",
    "\t\tself.norm = LayerNorm(layer.size)\n",
    "\t\n",
    "\tdef forward(self, x, memory, src_mask, tgt_mask):\n",
    "\t\tfor layer in layers:\n",
    "\t\t\tx = layer(x, memory, src_mask, tgt_mask)\n",
    "\t\treturn self.norm(x)\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "\tdef __init__(self, size, self_attn, cross_attn, feed_forward, dropout):\n",
    "\t\tsuper(DecoderLayer, self).__init__()\n",
    "\t\tself.size = size\n",
    "\t\tself.self_attn = self_attn\n",
    "\t\tself.cross_attn = cross_attn\n",
    "\t\tself.feed_forward = feed_forward\n",
    "\t\tself.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\t\n",
    "\tdef forward(self, x, memory, cross_mask, tgt_mask):\n",
    "\t\tm = memory\n",
    "\t\tx = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "\t\tx = self.sublayer[1](x, lambda x: self.cross_attn(x, m, m, cross_mask))\n",
    "\t\treturn self.sublayer[2](x, self.feed_forward)"
   ],
   "id": "23dd81e8a2b6dd8a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. 实现attention",
   "id": "9dacefa8c0cb5352"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:05:32.270809Z",
     "start_time": "2024-10-09T15:05:32.265139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def subsequent_mask(size):\n",
    "\tattn_shape = (1, size, size)\n",
    "\tsubsequent_mask = (torch.triu(torch.ones(attn_shape), diagonal=1).\n",
    "\t\t\t\t\t   type(torch.uint8))\n",
    "\treturn subsequent_mask == 0\n",
    "\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "\td_k = query.size(-1)\n",
    "\tscores = torch.matmul(query, key.transpose(-2, -1) / math.sqrt(d_k))\n",
    "\tif mask is not None:\n",
    "\t\tscores = scores.masked_fill(mask == 0, 1e-9)\n",
    "\tp_attn = scores.softmax(dim=-1)\n",
    "\tif droput is not None:\n",
    "\t\tp_attn = dropout(p_attn)\n",
    "\treturn torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "class MHA(nn.Module):\n",
    "\tdef __init__(self, h, d_model, dropout):\n",
    "\t\tsuper(MHA, self).__init__()\n",
    "\t\tassert d_model % h == 0\n",
    "\t\tself.d_k = d_model // h\n",
    "\t\tself.h = h\n",
    "\t\t# 为什么是clone(..., 4)?\n",
    "\t\t# 回答：四个线性映射，前三个对应 q k v\n",
    "\t\tself.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "\t\tself.attn = None\n",
    "\t\tself.dropout = nn.Dropout(p=dropout)\n",
    "\t\n",
    "\tdef forward(self, query, key, value, mask=None):\n",
    "\t\tif mask is not None:\n",
    "\t\t\tmask = mask.unsqueeze(1)\n",
    "\t\tbatch_num = query.size(0)\n",
    "\t\t\n",
    "\t\tquery, key, value = [\n",
    "\t\t\tlinear(x).view(batch_num, -1, self.h, self.d_k).transpose(1, 2)\n",
    "\t\t\tfor linear, x in zip(self.linears, (query, key, value))\n",
    "\t\t]\n",
    "\t\t\n",
    "\t\tx, self.attn = attention(\n",
    "\t\t\tquery, key, value, mask=mask, dropout=self.dropout\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tx = (\n",
    "\t\t\tx.transpose(1, 2)\n",
    "\t\t\t.contiguous()\n",
    "\t\t\t.view(batch_num, -1, self.h, self.d_k)\n",
    "\t\t)\n",
    "\t\tdel query\n",
    "\t\tdel key\n",
    "\t\tdel value\n",
    "\t\t\n",
    "\t\treturn self.linears[-1](x)"
   ],
   "id": "4ccff8b6b3b13391",
   "outputs": [],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
