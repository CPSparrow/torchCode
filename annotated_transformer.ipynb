{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## todo_list\n",
    "1. [ ] 是否已经实现了post_norm"
   ],
   "id": "76d891be6efe51d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. 导入相关的库",
   "id": "92bcabb2405d0565"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:24:08.945151Z",
     "start_time": "2024-10-14T00:24:08.942544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "id": "18508b8c6c5a456a",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. 实现相关的函数",
   "id": "7ca80e6950ecba9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:24:08.996630Z",
     "start_time": "2024-10-14T00:24:08.991924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clones(module, N: int):\n",
    "\treturn nn.ModuleList([deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "\tdef __init__(self, size, eps: float = 1e-5):\n",
    "\t\t\"\"\"eps的数值参考了pytorch的transformer实现\"\"\"\n",
    "\t\tsuper(LayerNorm, self).__init__()\n",
    "\t\tself.a_2 = nn.Parameter(torch.ones(size))\n",
    "\t\tself.b_2 = nn.Parameter(torch.zeros(size))\n",
    "\t\tself.eps = eps\n",
    "\t\t\n",
    "\t\t# 源代码上a_2和b_2没有作参数初始化，现在已经补充上\n",
    "\t\tnn.init.ones_(self.a_2)\n",
    "\t\tnn.init.zeros_(self.b_2)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tmean = x.mean(-1, keepdim=True)\n",
    "\t\tstd = x.std(-1, keepdim=True)\n",
    "\t\treturn self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "\tdef __init__(self, size, dropout):\n",
    "\t\tsuper(SublayerConnection, self).__init__()\n",
    "\t\tself.norm = LayerNorm(size)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\n",
    "\tdef forward(self, x, sublayer):\n",
    "\t\t# 看得出来这里的实现已经改为了pre-norm\n",
    "\t\treturn x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\tdef __init__(self, d_model: int, vocab_size: int):\n",
    "\t\tsuper(Generator, self).__init__()\n",
    "\t\tself.proj = nn.Linear(d_model, vocab_size)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\t# 用log_softmax代替softmax可以有效地避免数值溢出。此事在源码中亦有记载。\n",
    "\t\treturn F.log_softmax(self.project(x), dim=1)\n",
    "\n"
   ],
   "id": "7d7245ebf97920ab",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. 实现transformer",
   "id": "f8d2efc21ed1f2bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:24:09.047864Z",
     "start_time": "2024-10-14T00:24:09.040913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Transformer(nn.Module):\n",
    "\tdef __init__(self, encoder, decoder, src_emb, tgt_emb, generator):\n",
    "\t\t\"\"\"\n",
    "\t\t对整个模型结构的抽象。有关参数传递的设计与常见的有很大不同。\n",
    "\t\t因为EncoderLayer和DecoderLayer的设计基本相同，所以笔记就写在这里了：\n",
    "\t\t与torch的实现相比，我还是更喜欢源代码的设计。比如参数传递的方法以及对象的构造等等...\n",
    "\t\t也许以后会做更大规模的重构...\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Transformer, self).__init__()\n",
    "\t\tself.encoder = encoder\n",
    "\t\tself.decoder = decoder\n",
    "\t\tself.src_emb = src_emb\n",
    "\t\tself.tgt_emb = tgt_emb\n",
    "\t\tself.generator = generator\n",
    "\t\n",
    "\tdef forward(self, src, tgt, src_mask, tgt_mask):\n",
    "\t\t\"\"\"\n",
    "\t\ttodo： 也许可以考虑增加 src_mask 以及 tgt_mask 的默认值设计\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\t\n",
    "\tdef encode(self, src, src_mask):\n",
    "\t\treturn self.encoder(self.src_emb(src), src_mask)\n",
    "\t\n",
    "\tdef decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "\t\treturn self.decoder(self.tgt_emb(tgt), memory, src_mask, tgt_mask)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\tdef __init__(self, layer, N: int):\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\t\tself.layers = clones(layer, N)\n",
    "\t\tself.norm = LayerNorm(layer.size)\n",
    "\t\n",
    "\tdef forward(self, x, mask):\n",
    "\t\tfor layer in layers:\n",
    "\t\t\tx = layer(x, mask)\n",
    "\t\treturn self.norm(x)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "\tdef __init__(self, size, self_attn, feed_forward, dropout=0.1):\n",
    "\t\tsuper(EncoderLayer, self).__init__()\n",
    "\t\tself.size = size\n",
    "\t\tself.self_attn = self_attn\n",
    "\t\tself.feed_forward = feed_forward\n",
    "\t\tself.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "\t\n",
    "\tdef forward(self, x, mask):\n",
    "\t\tx = self.sublayer[0](x, lambda src: self.self_attn(src, src, src, mask))\n",
    "\t\treturn self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self, layer, N):\n",
    "\t\tsuper(Decoder, self).__init__()\n",
    "\t\tself.layers = clones(layer, N)\n",
    "\t\tself.norm = LayerNorm(layer.size)\n",
    "\t\n",
    "\tdef forward(self, x, memory, src_mask, tgt_mask):\n",
    "\t\tfor layer in layers:\n",
    "\t\t\tx = layer(x, memory, src_mask, tgt_mask)\n",
    "\t\treturn self.norm(x)\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "\tdef __init__(self, size, self_attn, cross_attn, feed_forward, dropout=0.1):\n",
    "\t\tsuper(DecoderLayer, self).__init__()\n",
    "\t\tself.size = size\n",
    "\t\tself.self_attn = self_attn\n",
    "\t\tself.cross_attn = cross_attn\n",
    "\t\tself.feed_forward = feed_forward\n",
    "\t\tself.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\t\n",
    "\tdef forward(self, x, memory, cross_mask, tgt_mask):\n",
    "\t\tm = memory\n",
    "\t\tx = self.sublayer[0](x, lambda tgt: self.self_attn(tgt, tgt, tgt, tgt_mask))\n",
    "\t\tx = self.sublayer[1](x, lambda tgt: self.cross_attn(tgt, m, m, cross_mask))\n",
    "\t\treturn self.sublayer[2](x, self.feed_forward)"
   ],
   "id": "23dd81e8a2b6dd8a",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. 实现attention",
   "id": "9dacefa8c0cb5352"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:24:09.098580Z",
     "start_time": "2024-10-14T00:24:09.093453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def subsequent_mask(seq_size: int):\n",
    "\t# 生成tgt使用的特殊mask\n",
    "\tattn_shape = (1, seq_size, seq_size)\n",
    "\tmask = (torch.triu(torch.ones(attn_shape), diagonal=1).\n",
    "\t\t\ttype(torch.uint8))\n",
    "\treturn mask == 0\n",
    "\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "\td_k = query.size(-1)\n",
    "\t# 自动使用batched matmul\n",
    "\tscores = torch.matmul(query, key.transpose(-2, -1) / math.sqrt(d_k))\n",
    "\tif mask is not None:\n",
    "\t\tscores = scores.masked_fill(mask == 0, 1e-9)\n",
    "\tp_attn = scores.softmax(dim=-1)\n",
    "\tif droput is not None:\n",
    "\t\tp_attn = dropout(p_attn)\n",
    "\treturn torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "class MHA(nn.Module):\n",
    "\tdef __init__(self, n_head: int, d_model: int, dropout: float = 0.1):\n",
    "\t\tsuper(MHA, self).__init__()\n",
    "\t\tassert d_model % h == 0\n",
    "\t\tself.d_k = d_model // n_head\n",
    "\t\tself.n_head = n_head\n",
    "\t\t# 提问：为什么是clone(..., 4)?\n",
    "\t\t# 回答：四个线性映射，前三个对应 q k v\n",
    "\t\t# 提问：在这里的实现中是简单的做了一个大linear再切分。这是否与\n",
    "\t\t#      通过n_head的多个linear等价？\n",
    "\t\t# (该问题暂时无法回答)\n",
    "\t\tself.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "\t\tself.attn = None\n",
    "\t\tself.dropout = nn.Dropout(p=dropout)\n",
    "\t\n",
    "\tdef forward(self, query, key, value, mask=None):\n",
    "\t\tif mask is not None:\n",
    "\t\t\tmask = mask.unsqueeze(1)\n",
    "\t\tbatch_num = query.size(0)\n",
    "\t\t\n",
    "\t\tquery, key, value = [\n",
    "\t\t\tlinear(x).view(batch_num, -1, self.h, self.d_k).transpose(1, 2)\n",
    "\t\t\tfor linear, x in zip(self.linears, (query, key, value))\n",
    "\t\t]\n",
    "\t\t\n",
    "\t\tx, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\t\t\n",
    "\t\tx = (\n",
    "\t\t\tx.transpose(1, 2)\n",
    "\t\t\t.contiguous()\n",
    "\t\t\t.view(batch_num, -1, self.h, self.d_k)\n",
    "\t\t)\n",
    "\t\tdel query\n",
    "\t\tdel key\n",
    "\t\tdel value\n",
    "\t\t\n",
    "\t\treturn self.linears[-1](x)"
   ],
   "id": "4ccff8b6b3b13391",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. 实现FFN\n",
    "> 如果看源码的话,可以看到linear的参数都已经做了初始化而且bias默认为True"
   ],
   "id": "f115cc6a975245d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:24:09.146572Z",
     "start_time": "2024-10-14T00:24:09.142886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FFN(nn.Module):\n",
    "\tdef __init__(self, d_model: int, d_ffn: int, dropout=0.1):\n",
    "\t\tsuper(FFN, self).__init__()\n",
    "\t\t# 注意到bias：default = true\n",
    "\t\tself.w_1 = nn.Linear(d_model, d_ffn)\n",
    "\t\tself.w_2 = nn.Linear(d_ffn, d_model)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\t# 不知道这个relu哪里来的\n",
    "\t\t# return self.w_2(self.dropout(self.w_1(x).relu()))\n",
    "\t\treturn self.w_2(self.dropout(nn.ReLU()(self.w_1(x))))"
   ],
   "id": "ee9abb03911c3add",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6. 实现 embedding 和 softmax \n",
    "> 参考源码，这里调整为log_softmax"
   ],
   "id": "6c28f9540ffb5c26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:24:09.203756Z",
     "start_time": "2024-10-14T00:24:09.194525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Embedding(nn.Module):\n",
    "\tdef __init__(self, vocab_size: int, d_model: int):\n",
    "\t\tsuper(Embedding, self).__init__()\n",
    "\t\tself.emb = nn.Embedding(vocab_size, d_model)\n",
    "\t\tself.d_model = d_model\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.emb(x) * math.sqrt(self.d_model)\n",
    "\n",
    "\n",
    "class PosEmb(nn.Module):\n",
    "\tdef __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "\t\tsuper(PosEmb, self).__init__()\n",
    "\t\tself.dropout = nn.Dropout(p=dropout)\n",
    "\t\t\n",
    "\t\tpe = torch.zeros(max_len, d_model)\n",
    "\t\tposition = torch.arange(0, max_len).unsqueeze(1)\n",
    "\t\tdiv_term = torch.exp(\n",
    "\t\t\ttorch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "\t\t)\n",
    "\t\tpe[:, 0::2] = torch.sin(position * div_term)\n",
    "\t\tpe[:, 1::2] = torch.cos(position * div_term)\n",
    "\t\tpe = pe.unsqueeze(0)\n",
    "\t\tself.register_buffer(\"pe\", pe)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = x + self.pe[:, :x.size(1)].requires_grad_(False)\n",
    "\t\treturn self.dropout(x)"
   ],
   "id": "f53162223ccfa8c9",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 测试：PosEmb的可视化",
   "id": "3cdda7307b274e22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:24:09.252520Z",
     "start_time": "2024-10-14T00:24:09.248672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import altair as alt\n"
   ],
   "id": "689793c9bf742cbb",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:24:09.301606Z",
     "start_time": "2024-10-14T00:24:09.297414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def example_positional():\n",
    "\t# 设置dropout=0以免图像撕裂\n",
    "\tpe = PosEmb(20, dropout=0)\n",
    "\ty = pe.forward(torch.zeros(1, 100, 20))\n",
    "\t\n",
    "\tdata = pd.concat(\n",
    "\t\t[\n",
    "\t\t\tpd.DataFrame(\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"embedding\": y[0, :, dim],\n",
    "\t\t\t\t\t\"dimension\": dim,\n",
    "\t\t\t\t\t\"position\": list(range(100)),\n",
    "\t\t\t\t}\n",
    "\t\t\t)\n",
    "\t\t\tfor dim in [4, 5, 6, 7]\n",
    "\t\t]\n",
    "\t)\n",
    "\t\n",
    "\treturn (\n",
    "\t\talt.Chart(data)\n",
    "\t\t.mark_line()\n",
    "\t\t.properties(width=800)\n",
    "\t\t.encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n",
    "\t\t.interactive()\n",
    "\t)\n",
    "\n",
    "# 取消下一行的注释就可以运行以查看可视化的结果\n",
    "# example_positional()"
   ],
   "id": "f5ef35fe894c010a",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 7. Full Model",
   "id": "42a67c8b437e0ca2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:24:09.351555Z",
     "start_time": "2024-10-14T00:24:09.347723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_model(\n",
    "\t\tsrc_vocab_size, tgt_vocab_size, n_layer=6, d_model=512, d_ffn=2048, n_head=8, dropout=0.1\n",
    "):\n",
    "\tattn = MHA(n_head, d_model, dropout)\n",
    "\tffn = FFN(d_model, d_ffn, dropout)\n",
    "\tpe = PosEmb(d_model, dropout=dropout)\n",
    "\tmodel = Transformer(\n",
    "\t\tEncoder(EncoderLayer(d_model, deepcopy(attn), deepcopy(ffn), dropout), n_layer),\n",
    "\t\tDecoder(DecoderLayer(d_model, deepcopy(attn), deepcopy(attn), deepcopy(ffn), dropout), n_layer),\n",
    "\t\tnn.Sequential(Embedding(len(src_vocab_size), d_model), deepcopy(pe)),\n",
    "\t\tnn.Sequential(Embedding(len(src_vocab_size), d_model), deepcopy(pe)),\n",
    "\t\tGenerator(d_model, tgt_vocab_size)\n",
    "\t)\n",
    "\t\n",
    "\t# 一个比较简单干脆的参数初始化方法。。。不过并不确定是哪些参数被重复初始化了。\n",
    "\tfor p in model.parameters():\n",
    "\t\tif p.dim() > 1:\n",
    "\t\t\tnn.init.xavier_uniform_(p)\n",
    "\treturn model\n",
    "\t"
   ],
   "id": "731b75f881898c69",
   "outputs": [],
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
