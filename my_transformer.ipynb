{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. 导入所需要的模块并设置语料的路径",
   "id": "be62bfcc8596d12c"
  },
  {
   "cell_type": "code",
   "id": "550baeb6bb0a4595",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:40:44.115094Z",
     "start_time": "2024-09-24T11:40:44.110826Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "en_name = './corpus/train/EN.txt'\n",
    "cn_name = './corpus/train/CN.txt'"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. 从文件中读取数据并且使用jieba分词，随后生成词表",
   "id": "d355ef4ffc27b817"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:40:44.163138Z",
     "start_time": "2024-09-24T11:40:44.159185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def file2tokens(file_name: str):\n",
    "    r\"\"\"\n",
    "    return lists of words and max len\n",
    "    \"\"\"\n",
    "    with open(file_name, 'r') as file:\n",
    "        data = file.read()\n",
    "        data = data.split('\\n')\n",
    "        data = [list(jieba.cut(t)) for t in data]\n",
    "        data = [t for t in data if len(t) > 5]\n",
    "        return data\n",
    "\n",
    "\n",
    "def tokens2vocab(data, target=None):\n",
    "    r\"\"\"\n",
    "    根据分词后的文本生成对应的词表(vocab)和相关数据\n",
    "    \n",
    "    :param data:generated by file2tokens\n",
    "    :param target: should be 'tgt' or 'src'\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if target == 'src':\n",
    "        vocab = {'P': 0}\n",
    "        for sentence in data:\n",
    "            for token in sentence:\n",
    "                if not token in vocab:\n",
    "                    vocab[token] = len(vocab)\n",
    "        idx2token = {i: t for i, t in enumerate(vocab)}\n",
    "        vocab_size = len(vocab)\n",
    "    elif target == 'tgt':\n",
    "        vocab = {'P': 0, 'S': 1, 'E': 2}\n",
    "        for sentence in data:\n",
    "            for token in sentence:\n",
    "                if not token in vocab:\n",
    "                    vocab[token] = len(vocab)\n",
    "        idx2token = {i: t for i, t in enumerate(vocab)}\n",
    "        vocab_size = len(vocab)\n",
    "    else:\n",
    "        raise ValueError(\"invalid param about target!\")\n",
    "    return vocab, idx2token, vocab_size"
   ],
   "id": "36436675051a50c1",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. 生成Transformer使用的LongTensor序列",
   "id": "bcd9e317e8bde32c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:40:44.215346Z",
     "start_time": "2024-09-24T11:40:44.208202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_data(src_tokens: list, tgt_tokens: list, src_vocab: dict, tgt_vocab: dict):\n",
    "    r\"\"\"\n",
    "    把分词后的文本转化成下标序列。本函数同时也要实现了padding的功能。\n",
    "    :param token_lists:输入由file2tokens生成的src_tokens和tgt_tokens\n",
    "    :param size: 分别需要作padding的长度\n",
    "    :return data_list: [LongTensor, LongTensor, LongTensor]\n",
    "    \"\"\"\n",
    "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
    "\n",
    "    def pad(x, max_len):\n",
    "        # 这里的padding的方法可能会有性能上的问题，但就先这样吧。\n",
    "        x = x + [0] * (max_len - len(x))\n",
    "        return x\n",
    "\n",
    "    src_len = max([len(sentence) for sentence in src_tokens])\n",
    "    tgt_len = max([len(sentence) for sentence in tgt_tokens])\n",
    "    print(\"tgt_len =\", tgt_len)\n",
    "    for src in src_tokens:\n",
    "        src_input = pad([src_vocab[token] for token in src], src_len)\n",
    "        enc_inputs.append(src_input)\n",
    "\n",
    "    for tgt in tgt_tokens:\n",
    "        tgt_input = pad([tgt_vocab[token] for token in tgt], tgt_len)\n",
    "        dec_inputs.append([tgt_vocab['S']] + tgt_input)\n",
    "        dec_outputs.append(tgt_input + [tgt_vocab['E']])\n",
    "\n",
    "    print(repr(enc_inputs[0]), len(enc_inputs[0]))\n",
    "    print(repr(dec_outputs[0]), len(dec_inputs[0]))\n",
    "    data_list = [torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)]\n",
    "    return data_list"
   ],
   "id": "9ff9d13186efa73e",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:40:44.325188Z",
     "start_time": "2024-09-24T11:40:44.320160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "src_tokens = file2tokens(en_name)\n",
    "src2idx, idx2src, src_vocab_size = tokens2vocab(src_tokens, 'src')\n",
    "\n",
    "tgt_tokens = file2tokens(cn_name)\n",
    "tgt2idx, idx2tgt, tgt_vocab_size = tokens2vocab(tgt_tokens, 'tgt')\n",
    "\n",
    "data_list = make_data(src_tokens, tgt_tokens, src2idx, tgt2idx)"
   ],
   "id": "ac2e2a5ce96e0142",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt_len = 62\n",
      "[1, 2, 3, 2, 4, 2, 5, 2, 6, 2, 7, 2, 8, 2, 9, 2, 10, 2, 11, 2, 12, 2, 13, 2, 14, 2, 15, 2, 16, 2, 17, 2, 18, 2, 19, 2, 20, 2, 21, 2, 22, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 97\n",
      "[3, 4, 5, 6, 7, 8, 9, 10, 5, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2] 63\n",
      "torch.Size([21, 97]) torch.Size([21, 63]) torch.Size([21, 63])\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "bec01d8eacf2749c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
