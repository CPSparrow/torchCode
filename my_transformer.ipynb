{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. 导入所需要的模块,设置语料的路径和训练时的batch_size",
   "id": "be62bfcc8596d12c"
  },
  {
   "cell_type": "code",
   "id": "550baeb6bb0a4595",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T12:01:59.136588Z",
     "start_time": "2024-09-24T12:01:59.131863Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 2\n",
    "en_name = './corpus/train/EN.txt'\n",
    "cn_name = './corpus/train/CN.txt'"
   ],
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. 从文件中读取数据并且使用jieba分词，随后生成词表",
   "id": "d355ef4ffc27b817"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T12:01:59.171609Z",
     "start_time": "2024-09-24T12:01:59.167506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def file2tokens(file_name: str):\n",
    "    r\"\"\"\n",
    "    return lists of words and max len\n",
    "    \"\"\"\n",
    "    with open(file_name, 'r') as file:\n",
    "        data = file.read()\n",
    "        data = data.split('\\n')\n",
    "        data = [list(jieba.cut(t)) for t in data]\n",
    "        data = [t for t in data if len(t) > 5]\n",
    "        return data\n",
    "\n",
    "\n",
    "def tokens2vocab(data, target=None):\n",
    "    r\"\"\"\n",
    "    根据分词后的文本生成对应的词表(vocab)和相关数据\n",
    "    \n",
    "    :param data:generated by file2tokens\n",
    "    :param target: should be 'tgt' or 'src'\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if target == 'src':\n",
    "        vocab = {'P': 0}\n",
    "        for sentence in data:\n",
    "            for token in sentence:\n",
    "                if not token in vocab:\n",
    "                    vocab[token] = len(vocab)\n",
    "        idx2token = {i: t for i, t in enumerate(vocab)}\n",
    "        vocab_size = len(vocab)\n",
    "    elif target == 'tgt':\n",
    "        vocab = {'P': 0, 'S': 1, 'E': 2}\n",
    "        for sentence in data:\n",
    "            for token in sentence:\n",
    "                if not token in vocab:\n",
    "                    vocab[token] = len(vocab)\n",
    "        idx2token = {i: t for i, t in enumerate(vocab)}\n",
    "        vocab_size = len(vocab)\n",
    "    else:\n",
    "        raise ValueError(\"invalid param about target!\")\n",
    "    return vocab, idx2token, vocab_size"
   ],
   "id": "36436675051a50c1",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. 生成Transformer使用的LongTensor序列",
   "id": "bcd9e317e8bde32c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T12:01:59.224004Z",
     "start_time": "2024-09-24T12:01:59.217036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_data(src_tokens: list, tgt_tokens: list, src_vocab: dict, tgt_vocab: dict):\n",
    "    r\"\"\"\n",
    "    把分词后的文本转化成下标序列。本函数同时也要实现了padding的功能。\n",
    "    :param token_lists:输入由file2tokens生成的src_tokens和tgt_tokens\n",
    "    :param size: 分别需要作padding的长度\n",
    "    :return data_list: [LongTensor, LongTensor, LongTensor]\n",
    "    \"\"\"\n",
    "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
    "\n",
    "    def pad(x, max_len):\n",
    "        # 这里的padding的方法可能会有性能上的问题，但就先这样吧。\n",
    "        x = x + [0] * (max_len - len(x))\n",
    "        return x\n",
    "\n",
    "    src_len = max([len(sentence) for sentence in src_tokens])\n",
    "    tgt_len = max([len(sentence) for sentence in tgt_tokens])\n",
    "    for src in src_tokens:\n",
    "        src_input = pad([src_vocab[token] for token in src], src_len)\n",
    "        enc_inputs.append(src_input)\n",
    "\n",
    "    for tgt in tgt_tokens:\n",
    "        tgt_input = pad([tgt_vocab[token] for token in tgt], tgt_len)\n",
    "        dec_inputs.append([tgt_vocab['S']] + tgt_input)\n",
    "        dec_outputs.append(tgt_input + [tgt_vocab['E']])\n",
    "\n",
    "    data_list = [torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)]\n",
    "    return data_list"
   ],
   "id": "9ff9d13186efa73e",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T12:01:59.280914Z",
     "start_time": "2024-09-24T12:01:59.269973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "src_tokens = file2tokens(en_name)\n",
    "src2idx, idx2src, src_vocab_size = tokens2vocab(src_tokens, 'src')\n",
    "\n",
    "tgt_tokens = file2tokens(cn_name)\n",
    "tgt2idx, idx2tgt, tgt_vocab_size = tokens2vocab(tgt_tokens, 'tgt')\n",
    "\n",
    "data_list = make_data(src_tokens, tgt_tokens, src2idx, tgt2idx)"
   ],
   "id": "ac2e2a5ce96e0142",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. 实现自己的DataSet类和DataLoader类",
   "id": "bdb83625a2e6ee6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T12:01:59.336300Z",
     "start_time": "2024-09-24T12:01:59.328900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyDataSet(Data.Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(MyDataSet, self).__init__()\n",
    "        self.enc_inputs, self.dec_inputs, self.dec_outputs = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.enc_inputs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n",
    "\n",
    "\n",
    "loader = Data.DataLoader(MyDataSet(data_list), batch_size, shuffle=True)\n",
    "for a, b, c in loader:\n",
    "    print(repr(b))\n",
    "    break"
   ],
   "id": "96dc931ba24d02d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1,  50,  38,  39,  38,  71,  15,  96, 125,  64,  68, 273, 274,   5,\n",
      "         275,  15, 276, 277, 278, 279, 280,  27, 281, 147, 208, 282,  32, 283,\n",
      "         284,  15,  30, 285,  38, 286, 140, 287,  38, 288,  38, 226, 289, 166,\n",
      "         290,  15, 131,  38, 291,  38, 292, 293,  21,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0],\n",
      "        [  1, 134,  15, 135, 136, 137, 138, 139,   8,  19,  18, 140,  20,  36,\n",
      "           5, 141,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0]])\n"
     ]
    }
   ],
   "execution_count": 82
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
