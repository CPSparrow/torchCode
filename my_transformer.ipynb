{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. 导入所需要的模块,设置语料的路径和训练时的batch_size",
   "id": "be62bfcc8596d12c"
  },
  {
   "cell_type": "code",
   "id": "550baeb6bb0a4595",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T13:42:42.058921Z",
     "start_time": "2024-09-24T13:42:40.870848Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 2\n",
    "en_name = './corpus/train/EN.txt'\n",
    "cn_name = './corpus/train/CN.txt'"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. 从文件中读取数据并且使用jieba分词，随后生成词表",
   "id": "d355ef4ffc27b817"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T13:42:42.073435Z",
     "start_time": "2024-09-24T13:42:42.069023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def file2tokens(file_name: str):\n",
    "    r\"\"\"\n",
    "    return lists of words and max len\n",
    "    \"\"\"\n",
    "    with open(file_name, 'r') as file:\n",
    "        data = file.read()\n",
    "        data = data.split('\\n')\n",
    "        data = [list(jieba.cut(t)) for t in data]\n",
    "        data = [t for t in data if len(t) > 5]\n",
    "        return data\n",
    "\n",
    "\n",
    "def tokens2vocab(data, target=None):\n",
    "    r\"\"\"\n",
    "    根据分词后的文本生成对应的词表(vocab)和相关数据\n",
    "    \n",
    "    :param data:generated by file2tokens\n",
    "    :param target: should be 'tgt' or 'src'\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if target == 'src':\n",
    "        vocab = {'P': 0}\n",
    "        for sentence in data:\n",
    "            for token in sentence:\n",
    "                if not token in vocab:\n",
    "                    vocab[token] = len(vocab)\n",
    "        idx2token = {i: t for i, t in enumerate(vocab)}\n",
    "        vocab_size = len(vocab)\n",
    "    elif target == 'tgt':\n",
    "        vocab = {'P': 0, 'S': 1, 'E': 2}\n",
    "        for sentence in data:\n",
    "            for token in sentence:\n",
    "                if not token in vocab:\n",
    "                    vocab[token] = len(vocab)\n",
    "        idx2token = {i: t for i, t in enumerate(vocab)}\n",
    "        vocab_size = len(vocab)\n",
    "    else:\n",
    "        raise ValueError(\"invalid param about target!\")\n",
    "    return vocab, idx2token, vocab_size"
   ],
   "id": "36436675051a50c1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. 生成Transformer使用的LongTensor序列",
   "id": "bcd9e317e8bde32c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T13:42:42.122204Z",
     "start_time": "2024-09-24T13:42:42.118435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_data(src_tokens: list, tgt_tokens: list, src_vocab: dict, tgt_vocab: dict):\n",
    "    r\"\"\"\n",
    "    把分词后的文本转化成下标序列。本函数同时也要实现了padding的功能。\n",
    "    :param token_lists:输入由file2tokens生成的src_tokens和tgt_tokens\n",
    "    :param size: 分别需要作padding的长度\n",
    "    :return data_list: [LongTensor, LongTensor, LongTensor]\n",
    "    \"\"\"\n",
    "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
    "\n",
    "    def pad(x, max_len):\n",
    "        # 这里的padding的方法可能会有性能上的问题，但就先这样吧。\n",
    "        x = x + [0] * (max_len - len(x))\n",
    "        return x\n",
    "\n",
    "    src_len = max([len(sentence) for sentence in src_tokens])\n",
    "    tgt_len = max([len(sentence) for sentence in tgt_tokens])\n",
    "    for src in src_tokens:\n",
    "        src_input = pad([src_vocab[token] for token in src], src_len)\n",
    "        enc_inputs.append(src_input)\n",
    "\n",
    "    for tgt in tgt_tokens:\n",
    "        tgt_input = pad([tgt_vocab[token] for token in tgt], tgt_len)\n",
    "        dec_inputs.append([tgt_vocab['S']] + tgt_input)\n",
    "        dec_outputs.append(tgt_input + [tgt_vocab['E']])\n",
    "\n",
    "    data_list = [torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)]\n",
    "    return data_list"
   ],
   "id": "9ff9d13186efa73e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T13:42:42.519952Z",
     "start_time": "2024-09-24T13:42:42.165988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "src_tokens = file2tokens(en_name)\n",
    "src2idx, idx2src, src_vocab_size = tokens2vocab(src_tokens, 'src')\n",
    "\n",
    "tgt_tokens = file2tokens(cn_name)\n",
    "tgt2idx, idx2tgt, tgt_vocab_size = tokens2vocab(tgt_tokens, 'tgt')\n",
    "\n",
    "data_list = make_data(src_tokens, tgt_tokens, src2idx, tgt2idx)"
   ],
   "id": "ac2e2a5ce96e0142",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.343 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. 实现自己的DataSet类和DataLoader类",
   "id": "bdb83625a2e6ee6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T13:42:42.531854Z",
     "start_time": "2024-09-24T13:42:42.527566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyDataSet(Data.Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(MyDataSet, self).__init__()\n",
    "        self.enc_inputs, self.dec_inputs, self.dec_outputs = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.enc_inputs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n",
    "\n",
    "\n",
    "loader = Data.DataLoader(MyDataSet(data_list), batch_size, shuffle=True)\n",
    "for a, b, c in loader:\n",
    "    print(repr(b))\n",
    "    break"
   ],
   "id": "96dc931ba24d02d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1, 194,  15, 195, 150,   5, 196, 180, 197, 198,  21,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0],\n",
      "        [  1,  50,  49,   8,  71,  15, 253, 254, 255, 256, 144,  12, 145, 147,\n",
      "           5, 257, 258, 259,   5, 260, 261, 147, 183,   5, 211, 262, 208, 262,\n",
      "          15, 263,  38, 246,  38, 264, 265, 266,  15, 263,  38, 245,  38, 264,\n",
      "         267, 266,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0]])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T13:42:42.643688Z",
     "start_time": "2024-09-24T13:42:42.641539Z"
    }
   },
   "cell_type": "code",
   "source": "print(tgt_vocab_size)",
   "id": "6db30f0aad6754da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
